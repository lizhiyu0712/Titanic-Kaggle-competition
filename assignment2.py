# -*- coding: utf-8 -*-
"""assignment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CIybYcs9miMs86toJG05qXEeO_gGoM8w
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""# Import other libraries"""

# Commented out IPython magic to ensure Python compatibility.
import sys
import matplotlib 
import scipy as sp 
import IPython
from IPython import display 

import sklearn 
import random
import time
from subprocess import check_output

#Common Model Algorithms
from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process
from sklearn import preprocessing

#Common Model Helpers
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn import feature_selection
from sklearn import model_selection
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score, roc_curve, auc

#Visualization
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.pylab as pylab
import seaborn as sns

#Configure Visualization Defaults
# %matplotlib inline
mpl.style.use('ggplot')
sns.set_style('white')
pylab.rcParams['figure.figsize'] = 12,8

import string
import warnings
warnings.filterwarnings('ignore')

"""# 1.Load data and check first few row of data"""

#Training data
train_data = pd.read_csv("/kaggle/input/titanic/train.csv")
train_data.head()

#Test data
test_data = pd.read_csv("/kaggle/input/titanic/test.csv")
test_data.head()

"""# 2. Exploratory data analysis and pre-processing

# 2.1 Data cleaning.
"""

#Check the age input
high_age = train_data['Age']>100 
print('Number of age above 100:', high_age.sum())
low_age  = train_data['Age']<0
print('Number of age below 0:',low_age.sum())

#Clean outliers
train_data[train_data['Fare'] > 300]
test_data[test_data['Fare'] > 300]

"""# 2.2 Identification and treatment of missing values and outliers"""

#Check the data to see the missing values and outliers
train_data.info()

#Drop the cabin column
train_data.drop(columns='Cabin', inplace = True)
train_data.dropna(inplace = True)

#Use median values to correct the other missing datas
train_data['Age'].fillna(train_data['Age'].median(), inplace = True)
train_data['Embarked'].fillna(train_data['Embarked'].mode()[0], inplace = True)
train_data['Fare'].fillna(train_data['Fare'].median(), inplace = True)

#Check the data again
train_data.info()

"""# 2.3 Feature engineering"""

#Change the name feature as title
train_test_data = [train_data, test_data] #Combining train and test dataset

for dataset in train_test_data:
    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)

train_data['Title'].value_counts()

#Change char label to numbers
num_title = {"Mr": 0, "Miss": 1, "Mrs": 2, 
                 "Master": 3, "Dr": 3, "Rev": 3, "Col": 3, "Major": 3, "Mlle": 3,"Countess": 3,
                 "Ms": 3, "Lady": 3, "Jonkheer": 3, "Don": 3, "Dona" : 3, "Mme": 3,"Capt": 3,"Sir": 3 }
for dataset in train_test_data:
    dataset['Title'] = dataset['Title'].map(num_title)

#Delete name
train_data.drop('Name', axis=1, inplace=True)
test_data.drop('Name', axis=1, inplace=True)
    
train_data.head()

#Change sex char value to numbers
num_sex = {"male": 0, "female": 1}
for dataset in train_test_data:
    dataset['Sex'] = dataset['Sex'].map(num_sex)
    
#Change Embarked char value to numbers   
num_embarked = {"S": 0, "C": 1, "Q": 2}
for dataset in train_test_data:
    dataset['Embarked'] = dataset['Embarked'].map(num_embarked)
    
train_data.head()

train_data['Fare'] = preprocessing.normalize([train_data['Fare']])[0]
train_data['Pclass'] = preprocessing.normalize([train_data['Pclass']])[0]
train_data['Age'] = preprocessing.normalize([train_data['Age']])[0]
train_data['SibSp'] = preprocessing.normalize([train_data['SibSp']])[0]

"""# 2.4 Plots describing different aspects of the data set"""

#Plot the correlation graph
fig, axs = plt.subplots(nrows=2, figsize=(25, 25))

sns.heatmap(train_data.drop(['PassengerId'], axis=1).corr(), ax=axs[0], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})
sns.heatmap(test_data.drop(['PassengerId'], axis=1).corr(), ax=axs[1], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})

for i in range(2):    
    axs[i].tick_params(axis='x', labelsize=15)
    axs[i].tick_params(axis='y', labelsize=15)
    
axs[0].set_title('Training Data Correlations Chart', size=20)
axs[1].set_title('Test Data Correlations Chart', size=20)

plt.show()

#Pclass and Age histograms
grid = sns.FacetGrid(train_data, col='Survived', row='Pclass', size=3, aspect=3)
grid.map(plt.hist, 'Age', alpha=1, bins=30)
grid.add_legend();

#Sex and Age histograms
grid = sns.FacetGrid(train_data, col='Survived', row='Sex', size=3, aspect=3)
grid.map(plt.hist, 'Age', alpha=1, bins=30)
grid.add_legend();

#Plot Fare outlier
fig, ax = plt.subplots(figsize=(5,5))
ax.scatter(train_data['Survived'],train_data['Fare'])
ax.set_xlabel('Survived')
ax.set_ylabel('Fare')
plt.show()

"""# 2.5 Print a basic data description"""

#Get info of the training data
train_data.info()

#Get info of the testing data
train_data.info()

"""# 2.6 Print descriptive statistics"""

#Get description of the data
train_data.describe()

test_data.describe()

"""# 3. Partition data """

#Partition data into train, validation and test sets
x = train_data.drop(['PassengerId', 'Survived', 'Ticket','SibSp'], axis=1)
y = train_data['Survived']

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=21)
print(x_train)
print("train_data Shape: {}".format(train_data.shape))
print("x Shape: {}".format(x.shape))
print("y Shape: {}".format(y.shape))
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

"""# 4. Fit models on the training set

# x_test and y_test will be used as validation set to train the data
"""

#Decision tree - non linear
decision_tree = DecisionTreeClassifier() 
decision_tree.fit(x_train, y_train)  
Y_pred = decision_tree.predict(x_test)  
acc_decision_tree = round(decision_tree.score(x_test, y_test) * 100, 2)

print("oob score:", acc_decision_tree, "%")

#Random forest - non liner
random_forest = RandomForestClassifier(n_estimators=100, oob_score = True)
random_forest.fit(x_train, y_train)
Y_prediction = random_forest.predict(x_test)
random_forest.score(x_train, y_train)
acc_random_forest = round(random_forest.score(x_test, y_test) * 100, 2)
print("oob score:", acc_random_forest, "%")

#Logistic regression - linear
logreg = LogisticRegression()
logreg.fit(x_train, y_train)
Y_pred = logreg.predict(x_test)
acc_log = round(logreg.score(x_test, y_test) * 100, 2)
print("oob score:", acc_log, "%")

"""# 5. Print the results of all three models on the test set"""

#Decision tree accuracy
from sklearn.model_selection import cross_val_predict
prediction_decision_tree= cross_val_predict(decision_tree, x_test, y_test, cv=3)

accuracy_score(y_test, prediction_decision_tree)

#Decision tree f1-score
f1_score(y_test, prediction_decision_tree)

#Decision tree AUC
roc_auc_score(y_test, prediction_decision_tree)

#Random forest accuracy
prediction_random_forest= cross_val_predict(random_forest, x_test, y_test, cv=3)

accuracy_score(y_test, prediction_random_forest)

#Random forest f1-score
f1_score(y_test, prediction_random_forest)

#Random forest AUC
roc_auc_score(y_test, prediction_random_forest)

#Logistic regression accuracy
from sklearn.model_selection import cross_val_predict
prediction_logreg= cross_val_predict(logreg, x_test, y_test, cv=3)

accuracy_score(y_test, prediction_logreg)

#Logistic regression f1-score
f1_score(y_test, prediction_logreg)

#Data processing for the test data
features = ["Pclass", "Sex", "Parch","Age", "Fare", "Embarked", "Title"]
test_data['Age'].fillna(test_data['Age'].median(), inplace = True)
test_data['Embarked'].fillna(test_data['Embarked'].mode()[0], inplace = True)
test_data['Fare'].fillna(test_data['Fare'].median(), inplace = True)
test_data['Fare'] = preprocessing.normalize([test_data['Fare']])[0]
test_data['Pclass'] = preprocessing.normalize([test_data['Pclass']])[0]
test_data['Age'] = preprocessing.normalize([test_data['Age']])[0]
test_data['SibSp'] = preprocessing.normalize([test_data['SibSp']])[0]

test_data[features]

"""# 6. Save the predictions of the best model on Kaggle's test set to submission.csv """

output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': random_forest.predict(test_data[features])})
output.to_csv('submission.csv', index=False)